{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "cv",
      "language": "python",
      "name": "cv"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Twenkid/Colab-Notebooks-AI-ML-CV/blob/main/SIFT-BRISK-ORG-BF-KNN_Image_Stitching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original source: https://colab.research.google.com/github/sthalles/computer-vision/blob/master/project-4/project-4.ipynb#scrollTo=UnqQA-5JE9gr\n",
        "\n",
        "Edited by Twenkid/Tosh 30.5.2025"
      ],
      "metadata": {
        "id": "T-SZBaVGd9Z6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZtd6pAQE9f7"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "cv2.ocl.setUseOpenCL(False)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PBeJwcME9f_"
      },
      "source": [
        "# select the image id (valid values 1,2,3, or 4)\n",
        "imageId = 1\n",
        "feature_extractor = 'sift' #'orb' #'brisk' # #'sift'  #'surf' - patented, lacking\n",
        "feature_matching = 'bf'\n",
        "feature_matching = \"knn\""
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qm_NcD0E9gC"
      },
      "source": [
        "# read images and transform them to grayscale\n",
        "# Make sure that the train image is the image that will be transformed\n",
        "#trainImg = cv2.imread('./input/foto' + str(imageId) + 'A.jpg')\n",
        "p1 = \"/content/A.jpg\"\n",
        "p2 = \"/content/B.jpg\"\n",
        "proot = \"/content\"\n",
        "p1 = r\"/content/\"\n",
        "p2 = r\"/content/\"\n",
        "f1 = \"A.jpg\"\n",
        "f2 = \"B.jpg\"\n",
        "f1 = \"012_03.jpg\"\n",
        "f2 = \"017_03.jpg\"\n",
        "f1 = \"c1.jpg\"\n",
        "f2 = \"c2.jpg\"\n",
        "f2 = \"B.jpg\"\n",
        "f1 = \"A.jpg\"\n",
        "f2 = \"B.jpg\"\n",
        "\n",
        "p1 = proot+\"/\"+f1\n",
        "p2 = proot+\"/\"+f2\n",
        "print(p1,p2)\n",
        "trainImg = cv2.imread(p1)\n",
        "trainImg = cv2.cvtColor(trainImg,cv2.COLOR_BGR2RGB)\n",
        "trainImg_gray = cv2.cvtColor(trainImg, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "#queryImg = cv2.imread('input/foto' + str(imageId) + 'B.jpg')\n",
        "queryImg = cv2.imread(p2)\n",
        "# Opencv defines the color channel in the order BGR.\n",
        "# Transform it to RGB to be compatible to matplotlib\n",
        "queryImg = cv2.cvtColor(queryImg,cv2.COLOR_BGR2RGB)\n",
        "queryImg_gray = cv2.cvtColor(queryImg, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(16,9))\n",
        "ax1.imshow(queryImg, cmap=\"gray\")\n",
        "ax1.set_xlabel(\"Query image\", fontsize=14)\n",
        "\n",
        "ax2.imshow(trainImg, cmap=\"gray\")\n",
        "ax2.set_xlabel(\"Train image (Image to be transformed)\", fontsize=14)\n",
        "\n",
        "#plt.savefig(\"./output/input_img_\"+str(imageId)+'.jpeg', bbox_inches='tight', dpi=300, optimize=True, format='jpeg')\n",
        "#plt.savefig(\"/content/input_img_\"+str(imageId)+'.jpeg', bbox_inches='tight', dpi=300, optimize=True, format='jpeg')\n",
        "plt.savefig(\"/content/input_img_\"+str(imageId)+'.jpeg', bbox_inches='tight', dpi=300, format='jpeg')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb0_FCAIE9gO"
      },
      "source": [
        "def detectAndDescribe(image, method=None):\n",
        "    \"\"\"\n",
        "    Compute key points and feature descriptors using an specific method\n",
        "    \"\"\"\n",
        "\n",
        "    assert method is not None, \"You need to define a feature detection method. Values are: 'sift', 'surf'\"\n",
        "\n",
        "    # detect and extract features from the image\n",
        "    if method == 'sift':\n",
        "        descriptor = cv2.xfeatures2d.SIFT_create()\n",
        "    elif method == 'surf':\n",
        "        descriptor = cv2.xfeatures2d.SURF_create()\n",
        "    elif method == 'brisk':\n",
        "        descriptor = cv2.BRISK_create()\n",
        "    elif method == 'orb':\n",
        "        descriptor = cv2.ORB_create()\n",
        "\n",
        "    # get keypoints and descriptors\n",
        "    (kps, features) = descriptor.detectAndCompute(image, None)\n",
        "\n",
        "    return (kps, features)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xowuFK7iE9gS"
      },
      "source": [
        "kpsA, featuresA = detectAndDescribe(trainImg_gray, method=feature_extractor)\n",
        "kpsB, featuresB = detectAndDescribe(queryImg_gray, method=feature_extractor)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfqfQCIuE9gU"
      },
      "source": [
        "# display the keypoints and features detected on both images\n",
        "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=False)\n",
        "ax1.imshow(cv2.drawKeypoints(trainImg_gray,kpsA,None,color=(0,255,0)))\n",
        "ax1.set_xlabel(\"(a)\", fontsize=14)\n",
        "ax2.imshow(cv2.drawKeypoints(queryImg_gray,kpsB,None,color=(0,255,0)))\n",
        "ax2.set_xlabel(\"(b)\", fontsize=14)\n",
        "\n",
        "#plt.savefig(\"./output/\" + feature_extractor + \"_features_img_\"+str(imageId)+'.jpeg', bbox_inches='tight',\n",
        "#            dpi=300, optimize=True, format='jpeg')\n",
        "plt.savefig(\"/content/\" + feature_extractor + \"_features_img_\"+str(imageId)+'.jpeg', bbox_inches='tight',\n",
        "            dpi=300, format='jpeg')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww8t8MhAE9gY"
      },
      "source": [
        "def createMatcher(method,crossCheck):\n",
        "    \"Create and return a Matcher Object\"\n",
        "\n",
        "    if method == 'sift' or method == 'surf':\n",
        "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=crossCheck)\n",
        "    elif method == 'orb' or method == 'brisk':\n",
        "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=crossCheck)\n",
        "    return bf"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIoACRVHE9ga"
      },
      "source": [
        "def matchKeyPointsBF(featuresA, featuresB, method):\n",
        "    bf = createMatcher(method, crossCheck=True)\n",
        "\n",
        "    # Match descriptors.\n",
        "    best_matches = bf.match(featuresA,featuresB)\n",
        "\n",
        "    # Sort the features in order of distance.\n",
        "    # The points with small distance (more similarity) are ordered first in the vector\n",
        "    rawMatches = sorted(best_matches, key = lambda x:x.distance)\n",
        "    print(\"Raw matches (Brute force):\", len(rawMatches))\n",
        "    return rawMatches"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2huykCrE9gd"
      },
      "source": [
        "def matchKeyPointsKNN(featuresA, featuresB, ratio, method):\n",
        "    bf = createMatcher(method, crossCheck=False)\n",
        "    # compute the raw matches and initialize the list of actual matches\n",
        "    rawMatches = bf.knnMatch(featuresA, featuresB, 2)\n",
        "    print(\"Raw matches (knn):\", len(rawMatches))\n",
        "    matches = []\n",
        "\n",
        "    # loop over the raw matches\n",
        "    for m,n in rawMatches:\n",
        "        # ensure the distance is within a certain ratio of each\n",
        "        # other (i.e. Lowe's ratio test)\n",
        "        if m.distance < n.distance * ratio:\n",
        "            matches.append(m)\n",
        "    return matches"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eHgWAorE9gf"
      },
      "source": [
        "print(\"Using: {} feature matcher\".format(feature_matching))\n",
        "\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "\n",
        "if feature_matching == 'bf':\n",
        "    matches = matchKeyPointsBF(featuresA, featuresB, method=feature_extractor)\n",
        "    img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,matches[:100],\n",
        "                           None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "elif feature_matching == 'knn':\n",
        "    matches = matchKeyPointsKNN(featuresA, featuresB, ratio=0.75, method=feature_extractor)\n",
        "    img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,np.random.choice(matches,100),\n",
        "                           None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "\n",
        "plt.imshow(img3)\n",
        "plt.axis('off')\n",
        "#plt.savefig(\"/content/\" + feature_matching + \"_matching_img_\"+str(imageId)+'.jpeg', bbox_inches='tight',\n",
        "#            dpi=300, optimize=True, format='jpeg')\n",
        "plt.savefig(\"/content/\" + feature_matching + \"_matching_img_\"+str(imageId)+'.jpeg', bbox_inches='tight',\n",
        "            dpi=300, format='jpeg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMNF_uEKE9gj"
      },
      "source": [
        "def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):\n",
        "    # convert the keypoints to numpy arrays\n",
        "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
        "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
        "\n",
        "    if len(matches) > 4:\n",
        "        # construct the two sets of points\n",
        "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
        "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
        "\n",
        "        # estimate the homography between the sets of points\n",
        "        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
        "            reprojThresh)\n",
        "\n",
        "        return (matches, H, status)\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__75LyOoE9gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69240979-6763-415c-c488-9355202418ba"
      },
      "source": [
        "M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)\n",
        "if M is None:\n",
        "    print(\"Error!\")\n",
        "(matches, H, status) = M\n",
        "print(H)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 9.61755181e-01  2.57764174e-01 -2.79548566e+02]\n",
            " [ 4.85792571e-02  1.14709604e+00 -1.81878876e+02]\n",
            " [ 7.52297239e-05  2.71621571e-04  1.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnqQA-5JE9gr"
      },
      "source": [
        "# Apply a horizontal panorama\n",
        "width = queryImg.shape[1] + trainImg.shape[1]\n",
        "height = max(queryImg.shape[0], trainImg.shape[0])\n",
        "# otherwise, apply a perspective warp to stitch the images\n",
        "# together\n",
        "(matches, H, status) = M\n",
        "result = cv2.warpPerspective(trainImg, H,\n",
        "    (width, height))\n",
        "result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.axis('off')\n",
        "plt.imshow(result)\n",
        "\n",
        "#imageio.imwrite(\"./output/horizontal_panorama_img_\"+str(imageId)+'.jpeg', result)\n",
        "imageio.imwrite(\"/content/horizontal_panorama_img_\"+str(imageId)+'.jpeg', result)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgBHhSxsE9go",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a23bb639-69f0-443e-d20f-3bcfe8aeedb2"
      },
      "source": [
        "# Apply a vertical panorama\n",
        "width = max(trainImg.shape[1], queryImg.shape[1])\n",
        "height = trainImg.shape[0] + queryImg.shape[0]\n",
        "\n",
        "result = cv2.warpPerspective(trainImg, H, (width, height))\n",
        "result[0:queryImg.shape[0], :] = queryImg\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(result)\n",
        "\n",
        "imageio.imwrite(\"./output/vertical_panorama_img_\"+str(imageId)+'.jpeg', result)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (768,1024,3) into shape (768,1280,3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-6c780be28710>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpPerspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainImg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mqueryImg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueryImg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (768,1024,3) into shape (768,1280,3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apI2F77HE9gt"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCtyJntxE9gw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}