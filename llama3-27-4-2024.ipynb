{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423982b1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-29T11:03:47.101247Z",
     "iopub.status.busy": "2024-07-29T11:03:47.100485Z",
     "iopub.status.idle": "2024-07-29T11:03:47.787347Z",
     "shell.execute_reply": "2024-07-29T11:03:47.786587Z"
    },
    "papermill": {
     "duration": 0.698022,
     "end_time": "2024-07-29T11:03:47.789600",
     "exception": false,
     "start_time": "2024-07-29T11:03:47.091578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4853f803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:03:47.805789Z",
     "iopub.status.busy": "2024-07-29T11:03:47.805451Z",
     "iopub.status.idle": "2024-07-29T11:04:12.990113Z",
     "shell.execute_reply": "2024-07-29T11:04:12.989179Z"
    },
    "papermill": {
     "duration": 25.195111,
     "end_time": "2024-07-29T11:04:12.992362",
     "exception": false,
     "start_time": "2024-07-29T11:03:47.797251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (21.3)\r\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (1.11.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging) (3.1.1)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install packaging ninja\n",
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bc4205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:04:13.010553Z",
     "iopub.status.busy": "2024-07-29T11:04:13.009689Z",
     "iopub.status.idle": "2024-07-29T11:04:13.357766Z",
     "shell.execute_reply": "2024-07-29T11:04:13.356621Z"
    },
    "papermill": {
     "duration": 0.358474,
     "end_time": "2024-07-29T11:04:13.359086",
     "exception": true,
     "start_time": "2024-07-29T11:04:13.000612",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m help(\u001b[43mtransformers\u001b[49m\u001b[38;5;241m.\u001b[39mpipeline)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "help(transformers.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16de786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T12:18:23.065113Z",
     "iopub.status.busy": "2024-05-04T12:18:23.064746Z",
     "iopub.status.idle": "2024-05-04T12:18:35.434769Z",
     "shell.execute_reply": "2024-05-04T12:18:35.433634Z",
     "shell.execute_reply.started": "2024-05-04T12:18:23.065077Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d8cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T12:18:35.437063Z",
     "iopub.status.busy": "2024-05-04T12:18:35.436768Z",
     "iopub.status.idle": "2024-05-04T12:18:36.858468Z",
     "shell.execute_reply": "2024-05-04T12:18:36.857120Z",
     "shell.execute_reply.started": "2024-05-04T12:18:35.437035Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0452b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47066b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:37:58.579532Z",
     "iopub.status.busy": "2024-05-03T15:37:58.579185Z",
     "iopub.status.idle": "2024-05-03T15:37:59.563785Z",
     "shell.execute_reply": "2024-05-03T15:37:59.562789Z",
     "shell.execute_reply.started": "2024-05-03T15:37:58.579499Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40071fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:37:59.565579Z",
     "iopub.status.busy": "2024-05-03T15:37:59.565267Z",
     "iopub.status.idle": "2024-05-03T15:37:59.571582Z",
     "shell.execute_reply": "2024-05-03T15:37:59.570747Z",
     "shell.execute_reply.started": "2024-05-03T15:37:59.565549Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868b898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T12:18:36.860460Z",
     "iopub.status.busy": "2024-05-04T12:18:36.860088Z",
     "iopub.status.idle": "2024-05-04T12:18:49.606418Z",
     "shell.execute_reply": "2024-05-04T12:18:49.605303Z",
     "shell.execute_reply.started": "2024-05-04T12:18:36.860425Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994ce83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T12:24:27.440185Z",
     "iopub.status.busy": "2024-05-04T12:24:27.439230Z",
     "iopub.status.idle": "2024-05-04T12:29:12.572503Z",
     "shell.execute_reply": "2024-05-04T12:29:12.571528Z",
     "shell.execute_reply.started": "2024-05-04T12:24:27.440137Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a computer expert, hardware, electronics, computer history. Computer chronicles, Byte magazine. \"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the best 8-bit CPU before 1974 and why?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde6766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T12:34:02.999431Z",
     "iopub.status.busy": "2024-05-04T12:34:02.997837Z",
     "iopub.status.idle": "2024-05-04T12:42:22.053891Z",
     "shell.execute_reply": "2024-05-04T12:42:22.052946Z",
     "shell.execute_reply.started": "2024-05-04T12:34:02.999379Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a computer expert, hardware, electronics, computer history. Computer chronicles, Byte magazine. \"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the best 16-bit CPU between 1995 and 1996 and why? What was the price? How it compared with the 32-bit CPUs?\"},\n",
    "]\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=500,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    streamer = streamer\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae3c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T12:43:44.850484Z",
     "iopub.status.busy": "2024-05-04T12:43:44.849748Z",
     "iopub.status.idle": "2024-05-04T12:45:25.243062Z",
     "shell.execute_reply": "2024-05-04T12:45:25.242158Z",
     "shell.execute_reply.started": "2024-05-04T12:43:44.850446Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a computer expert, hardware, electronics, computer history. Computer chronicles, Byte magazine. \"},\n",
    "    {\"role\": \"user\", \"content\": \"What about 386SX? Isn't DX a 32-bit CPU?\"},\n",
    "]\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    streamer = streamer\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02178f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "#pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "pipeline = transformers.pipeline(\"text-generation\", max_new_tokens = 200,  do_sample=True, temperature=1.5,top_p=0.9, streamer = streamer, model=model_id, model_kwargs={\"torch_dtype\": torch.float16}, device_map=\"auto\")\n",
    "s = pipeline(\"Hey how are you doing today?\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312db450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:52:35.026665Z",
     "iopub.status.busy": "2024-05-03T15:52:35.026278Z",
     "iopub.status.idle": "2024-05-03T15:53:48.033365Z",
     "shell.execute_reply": "2024-05-03T15:53:48.032443Z",
     "shell.execute_reply.started": "2024-05-03T15:52:35.026632Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "pipeline(\"What's the best 8-bit CPU before 1974 and why?\", streamer=streamer, max_new_tokens=1000,do_sample=True, temperature=0.7, top_k=40, repetition_penalty=1.0, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83a3ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:54:44.383673Z",
     "iopub.status.busy": "2024-05-03T15:54:44.383272Z",
     "iopub.status.idle": "2024-05-03T15:54:52.099340Z",
     "shell.execute_reply": "2024-05-03T15:54:52.098459Z",
     "shell.execute_reply.started": "2024-05-03T15:54:44.383642Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "prompt=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ you are C++ game programmer, HLSL, shaders, Direct X. }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ Write a shader for DirectX9C for bump mapping and multitexturing. Show the C++ code as well. }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "pipeline(prompt, streamer=streamer, max_new_tokens=100,do_sample=True, temperature=0.7, top_k=40, repetition_penalty=1.0, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20247f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:58:25.907572Z",
     "iopub.status.busy": "2024-05-03T15:58:25.906750Z",
     "iopub.status.idle": "2024-05-03T15:58:30.446627Z",
     "shell.execute_reply": "2024-05-03T15:58:30.445582Z",
     "shell.execute_reply.started": "2024-05-03T15:58:25.907541Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query1 = \"Write a shader for DirectX9C for bump mapping and multitexturing. Show the C++ code as well.\"\n",
    "messages1 = [\n",
    " {\"role\": \"system\", \"content\": \"You are a programmer, game developer. DirectX game development, GPU, programming, C++.\"},\n",
    "    {\"role\": \"user\", \"content\": query1}\n",
    "]\n",
    "pipeline(prompt, streamer=streamer, max_new_tokens=1000,do_sample=True, temperature=1.0, top_k=40, repetition_penalty=1.25, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70751877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:55:39.713700Z",
     "iopub.status.busy": "2024-05-03T15:55:39.713358Z",
     "iopub.status.idle": "2024-05-03T15:55:57.927315Z",
     "shell.execute_reply": "2024-05-03T15:55:57.925765Z",
     "shell.execute_reply.started": "2024-05-03T15:55:39.713671Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "you are C++ game programmer, HLSL, shaders, Direct X.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Write a shader for DirectX9C for bump mapping and multitexturing. Show the C++ code as well.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "pipeline(prompt, streamer=streamer, max_new_tokens=300,do_sample=True, temperature=0.7, top_k=40, repetition_penalty=1.0, top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7a749",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003399f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:46:46.304358Z",
     "iopub.status.busy": "2024-05-03T15:46:46.303673Z",
     "iopub.status.idle": "2024-05-03T15:46:46.993022Z",
     "shell.execute_reply": "2024-05-03T15:46:46.991826Z",
     "shell.execute_reply.started": "2024-05-03T15:46:46.304328Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = cuda.list_devices()\n",
    "gpus\n",
    "for i in gpus: i.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f52da2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-01T00:54:15.916381Z",
     "iopub.status.idle": "2024-05-01T00:54:15.916699Z",
     "shell.execute_reply": "2024-05-01T00:54:15.916555Z",
     "shell.execute_reply.started": "2024-05-01T00:54:15.916541Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install numba\n",
    "\n",
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639cd28",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#del pipeline\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Assuming `obj` is the object consuming GPU memory\n",
    "obj = None\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "\n",
    "# Empty PyTorch cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87acecc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf25f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T00:59:53.053184Z",
     "iopub.status.busy": "2024-05-01T00:59:53.052733Z",
     "iopub.status.idle": "2024-05-01T01:01:04.780835Z",
     "shell.execute_reply": "2024-05-01T01:01:04.779759Z",
     "shell.execute_reply.started": "2024-05-01T00:59:53.053120Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = r\"\"\"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "from .autonotebook import tqdm as notebook_tqdm\"\n",
    "\"\"\"\n",
    "\n",
    "query = \"Кои са петте най-високоплатени български спортисти, музиканти и шофьори на камиони? Бъди подробен. Отговори на български език.\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are translator.Answer in Bulgarian. Разкажи за политическите скандали от 2013 до 2023 г. в България на български език.\"},    \n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are translator.Answer in Bulgarian. Разкажи за политическите скандали от 2013 до 2023 г. Кои политици участват в тях от кои партии? В България на български език.\"},    \n",
    "    {\"role\": \"user\", \"content\": query},   \n",
    "]\n",
    "query = \"Кои са петте най-високоплатени български спортисти, музиканти и шофьори на камиони? Бъди подробен. Отговори на български език.\"\n",
    "messages = [\n",
    " {\"role\": \"system\", \"content\": \"Ти си български журналист, медии, sportal, btv, БНТ, новини, спортни новини заплащане. Отговаряй на български език.\"},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "\n",
    "query1 = \"Compare the instruction sets of Z80, 80386 and PowerPC processors. Which one has floating point operations?\"\n",
    "messages1 = [\n",
    " {\"role\": \"system\", \"content\": \"Computers, PCs, hardware, CPU, microprocessor, programming.\"},\n",
    "    {\"role\": \"user\", \"content\": query1}\n",
    "]\n",
    "\n",
    "query2 = \"What's the fastest 8-bit CPU running on up to 5 MHz? In which computers it was used?\"\n",
    "messages2 = [\n",
    " {\"role\": \"system\", \"content\": \"Computers, PCs, hardware, CPU, microprocessor, programming.\"},\n",
    "    {\"role\": \"user\", \"content\": query2}\n",
    "]\n",
    "messagess_arr = [messages1, messages2]\n",
    "\n",
    "#messages = [\n",
    "# {\"role\": \"system\", \"content\": \"Как да кандидатствам в Софийския университетКои са десетте най-високоплатени български спортисти, музиканти и шофьори? Бъди подробен. Answer in Bulgarian language. Translate to Bulgarian\"},    \n",
    "#    {\"role\": \"user\", \"content\": query}\n",
    "#]\n",
    "#    {\"role\": \"system\", \"content\": \"Кой е Йордан Йовчев? Какви медали е печелил на кои олимпиади? За коя страна се състезава?\"},    \n",
    "#   {\"role\": \"user\", \"content\": query}\n",
    "\n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ you are C++ game programmer, HLSL, shaders, Direct X. }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ Write a shader for DirectX9C for bump mapping and multitexturing. Show the C++ code as well. }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=1000,do_sample=True, temperature=0.7, top_k=40, repetition_penalty=1.0, top_p=0.9)\n",
    "#generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=200,do_sample=True, temperature=5.0, top_k=40, repetition_penalty=1.15, top_p=0.9)\n",
    "#generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=250,do_sample=True, temperature=0.9, top_k=40, repetition_penalty=1.2, top_p=0.9)\n",
    "#f.write(\"\\n\"+get_time_string()+\"\\n???\")\n",
    "#f.write(prompt)\n",
    "#f.write(tokenizer.batch_decode(generated_ids)[0])\n",
    "#f.flush()\n",
    "out = tokenizer.batch_decode(generated_ids)[0]\n",
    "out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for messages in messagess_arr:\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d77be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:39:20.785838Z",
     "iopub.status.busy": "2024-04-27T22:39:20.784768Z",
     "iopub.status.idle": "2024-04-27T22:39:20.790769Z",
     "shell.execute_reply": "2024-04-27T22:39:20.789843Z",
     "shell.execute_reply.started": "2024-04-27T22:39:20.785791Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dir(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca21f149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T22:39:26.290382Z",
     "iopub.status.busy": "2024-04-27T22:39:26.289507Z",
     "iopub.status.idle": "2024-04-27T22:39:26.917467Z",
     "shell.execute_reply": "2024-04-27T22:39:26.916212Z",
     "shell.execute_reply.started": "2024-04-27T22:39:26.290349Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(outputs[1][\"generated_text\"][len(prompt):])\n",
    "#print(outputs[2][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bea69a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tqdm progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb38df4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd2201",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# pretrained_model_name_or_path=\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\",\n",
    "#path = \"codellama/CodeLlama-7b-Instruct-hf\" #TabbyML/CodeLlama-7B\"\n",
    "path = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.2\"\n",
    "#No, transformers don't support GGUF? for local running\n",
    "# path = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.2-GGUF/BgGPT-7B-Instruct-v0.2.Q4_K_M.gguf\"\n",
    "#path = BgGPT-7B-Instruct-v0.2-GGUF/BgGPT-7B-Instruct-v0.2.Q4_K_M.gguf\n",
    "# path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=path, #INSAIT-Institute/BgGPT-7B-Instruct-v0.2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    #device_map=\"auto\", #+\n",
    "    #load_in_8bit=True #deprecated\n",
    "    #quantization_config = quantization_config\n",
    "    #use_flash_attention_2 = True  #Only for Geforce 3000+ etc.\n",
    ")\n",
    "#fails to use quantization_config, load_in_8bit - deprecated\n",
    "#use_flash_attn_2=True,\n",
    "#If using GPU, uncomment low_cpu_mem\n",
    "#If using CPU: comment it (but probably out of memory... it couldn't fit the model)\n",
    "#device_map=\"auto\",\n",
    "#low_cpu_mem_usage=True, # or crashes\n",
    "#use_flash_attn_2=True,\n",
    "# mistralai/Mistral-7B-Instruct-v0.1\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.250454,
   "end_time": "2024-07-29T11:04:13.684536",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-29T11:03:44.434082",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
